Metadata-Version: 2.4
Name: tlc_pipeline
Version: 0.0.1
Summary: Integrar registros de viagens de tÃ¡xi da cidade de Nova York de janeiro a maio de 2023 no ifood Data Lake
Author-email: Debora <deboraellenss2@gmail.com>
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: pyspark==3.4.1
Requires-Dist: great_expectations
Requires-Dist: boto3
Provides-Extra: dev
Requires-Dist: pytest; extra == "dev"
Requires-Dist: black; extra == "dev"
Requires-Dist: flake8; extra == "dev"
Requires-Dist: pip-tools; extra == "dev"

# ğŸš• TLC Pipeline Documentation

## ğŸŒŸ Project Overview

This project implements a data pipeline for processing New York City Taxi and Limousine Commission (TLC) trip data. The pipeline handles data ingestion, transformation, and storage in a structured format.

## ğŸ—ï¸ Pipeline Architecture

The pipeline consists of several key components:

1. **ğŸ“¥ Raw Data Ingestion**: Reads source data files (Parquet format)
2. **ğŸ”„ Column Mapping**: Maps different column names to standardized formats
3. **âš™ï¸ Data Transformation**: Processes and standardizes the data
4. **ğŸ’¾ Data Storage**: Writes processed data to a structured storage system

## ğŸ§© Key Components

### ğŸ“¥ Raw Ingestion

The raw ingestion process reads Parquet files from a source location and prepares them for further processing.

#### ğŸ”„ Column Mapping (De-Para)

The pipeline includes a sophisticated column mapping system that:

- ğŸ“ Normalizes column names across different data sources
- ğŸ” Handles special/exclusive columns that need individual treatment
- ğŸ”— Groups similar columns based on similarity metrics

```python
# Example usage:
de_para = sugerir_de_para(spark, "path/to/parquet/files", limite_similaridade=0.8)
```

#### ğŸ› ï¸ Column Mapping Functions

1. **`sugerir_de_para()`**: Analyzes Parquet files to suggest column mappings based on similarity
                         - ğŸš« Handles error files by moving them to quarantine
                         - ğŸ·ï¸ Supports exclusive columns that should not be grouped
                         - ğŸ” Uses fuzzy matching to identify similar columns

2. **`corrigir_de_para_colunas_exclusivas()`**: Refines column mappings by separating exclusive columns
                         - âœ… Ensures mandatory columns like "tpep_pickup_datetime" and "vendorid" are included
                         - ğŸ”€ Creates individual mappings for exclusive columns

3. **`exportar_de_para_csv()`** and **`exportar_de_para_json()`**: Export column mappings to CSV or JSON formats for reference and auditing

### ğŸ’¾ Data Writing

The `write_to_raw_bucket()` function handles writing processed data to the destination storage:

- ğŸ“Š Partitions data by year and month for efficient querying
- ğŸ›¡ï¸ Handles empty dataframes gracefully
- âš ï¸ Provides error handling and logging
- ğŸ”„ Supports both local and S3 paths

```python
# Example usage:
success = write_to_raw_bucket(processed_df, "s3://bucket/path/to/output")
```

Key features:
- ğŸ“‚ Data is partitioned by source_type, year, and month
- âš¡ Uses 20 partitions for balanced distribution
- ğŸ”„ Overwrites existing data in the target location

## ğŸ”„ Data Flow

1. ğŸ“¥ Source Parquet files are read
2. ğŸ”„ Column names are normalized and mapped using the de-para system
3. âš™ï¸ Data is transformed and standardized
4. ğŸ’¾ Processed data is written to the destination with proper partitioning

## âš ï¸ Error Handling

The pipeline includes robust error handling:
- ğŸ”’ Files that cannot be processed are moved to a quarantine directory
- ğŸ“ Detailed error messages are logged
- ğŸ›¡ï¸ Empty dataframes are handled gracefully

## ğŸš€ Usage Examples

```python
# 1. Generate column mappings
de_para = sugerir_de_para(spark, "input_data/", 
                                               limite_similaridade=0.8,
                                               colunas_exclusivas=["special_column1", "special_column2"])

# 2. Refine mappings for exclusive columns
refined_de_para = corrigir_de_para_colunas_exclusivas(de_para, 
                                                                           colunas_exclusivas=["special_column1", "special_column2"])

# 3. Export mappings for reference
exportar_de_para_json(refined_de_para, "mappings/column_mappings.json")
exportar_de_para_csv(refined_de_para, "mappings/column_mappings.csv")

# 4. Process and write data
write_to_raw_bucket(processed_df, "s3://raw-bucket/tlc_data/")
```

## ğŸ’¡ Best Practices

- ğŸ‘€ Always review the generated column mappings before using them in production
- ğŸ” Monitor the quarantine directory for problematic files
- ğŸ›ï¸ Adjust the similarity threshold based on your data characteristics
- âœ… Include all mandatory columns in your mapping

## ğŸ“Š Project Structure

```
src/
â”œâ”€â”€ tlc_pipeline/
â”‚   â”œâ”€â”€ raw_ingestion/
â”‚   â”‚   â”œâ”€â”€ de_para.py        # Column mapping functionality
â”‚   â”‚   â”œâ”€â”€ write_to_raw_bucket.py  # Data writing functionality
```

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.
```

To create this file, run:

```bash
mkdir -p $(dirname README.md)
touch README.md
