import os
import logging
from typing import Dict, List, Optional

from pyspark.sql import DataFrame, SparkSession
from pyspark.sql.functions import col
from pyspark.sql.utils import AnalysisException

from raw_ingestion.normalize_schema import ler_e_padronizar_parquets
from raw_ingestion.de_para import de_para
from config import INPUT_BUCKET
from utils.helpers import get_spark_session, write_to_bucket, setup_logging



logger = setup_logging()


class BronzeLayerIngestion:
    """
    Handles ingestion of raw taxi data to bronze layer standards.
    
    This class orchestrates the end-to-end process of reading raw TLC trip data files,
    normalizing their schema to a consistent format, extracting temporal dimensions
    for partitioning, and writing the results to the bronze layer storage.
    
    The bronze layer maintains the original granularity and content of the data
    while standardizing the format and structure for downstream processing.
    
    Attributes:
        spark (SparkSession): The Spark session used for data processing
        input_path (str): Path to the directory containing raw TLC data files
        output_path (str): Destination path for the bronze layer data
        PARTITION_COLUMNS (List[str]): Columns used for partitioning the output data
        DEFAULT_INPUT_PATH (str): Default location of raw data files
        DEFAULT_OUTPUT_PATH (str): Default destination for bronze layer data
    """
    
    # Default paths
    DEFAULT_INPUT_PATH = "src/tlc_pipeline/data/tlc_trip_record_data/"
    DEFAULT_OUTPUT_PATH = INPUT_BUCKET
    
    # Default partition columns
    PARTITION_COLUMNS = ["source_type", "year", "month"]
    
    def __init__(
        self, 
        spark: SparkSession = None,
        input_path: str = DEFAULT_INPUT_PATH,
        output_path: str = DEFAULT_OUTPUT_PATH
    ):
        """
        Initialize the bronze layer ingestion pipeline.
        
        Sets up the processing environment with the specified Spark session and paths.
        If no Spark session is provided, one will be created with appropriate configurations
        for S3/MinIO connectivity.
        
        Args:
            spark: Existing SparkSession to use. If None, a new session will be created.
            input_path: Directory path containing the raw TLC trip data files.
            output_path: Destination path where bronze layer data will be written.
                         Typically an S3 bucket or MinIO location.
        """
        self.spark = spark or get_spark_session()
        self.input_path = input_path
        self.output_path = output_path
        logger.info(f"BronzeLayerIngestion initialized with input_path={input_path}, output_path={output_path}")
    
    def get_mapping_dictionary(self) -> Dict:
        """
        Generate a column mapping dictionary for schema normalization.
        
        Analyzes the raw data files to identify schema variations and creates a mapping
        dictionary that translates source column names to standardized column names.
        This handles differences in column naming across different file formats and
        time periods in the TLC dataset.
        
        Returns:
            Dictionary mapping source column names to standardized column names.
            For example: {'VendorID': 'vendorid', 'passenger_count': 'passenger_count'}
            
        Raises:
            Exception: If the mapping dictionary cannot be generated due to file access
                      issues or unexpected file formats.
        """
        try:
            logger.info("Generating column mapping dictionary")
            mapping_dict = de_para(self.spark, self.input_path)
            logger.debug(f"Generated mapping dictionary: {mapping_dict}")
            return mapping_dict
        except Exception as e:
            logger.error(f"Failed to generate mapping dictionary: {str(e)}")
            raise
    
    def read_and_normalize_data(self, mapping_dict: Dict) -> Optional[DataFrame]:
        """
        Read raw data files and normalize their schema to a consistent format.
        
        This method:
        1. Reads all Parquet files from the input directory
        2. Applies the mapping dictionary to standardize column names
        3. Ensures a consistent schema across all files regardless of source format
        
        The normalization process handles differences in column names, data types,
        and structure across different versions of the TLC dataset.
        
        Args:
            mapping_dict: Dictionary mapping source column names to standardized names,
                         as generated by get_mapping_dictionary()
            
        Returns:
            DataFrame with normalized schema, or None if no data is found
            
        Raises:
            AnalysisException: If there are issues reading the Parquet files
            ValueError: If the input files have unexpected format or content
            Exception: For other unexpected errors during processing
        """
        try:
            logger.info(f"Reading and normalizing data from {self.input_path}")
            
            # Check if input path exists
            if not os.path.exists(self.input_path):
                logger.error(f"Input path does not exist: {self.input_path}")
                return None
                
            # Read and normalize data
            df = ler_e_padronizar_parquets(self.spark, self.input_path, mapping_dict)
            
            # Check if dataframe is empty
            if df.rdd.isEmpty():
                logger.warning("No data found in input files")
                return None
                
            logger.info(f"Successfully read and normalized data: {df.count()} rows")
            return df
        except AnalysisException as e:
            logger.error(f"Failed to read or normalize data: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error during data reading/normalization: {str(e)}")
            raise
    
    def extract_date_parts(self, df: DataFrame) -> DataFrame:
        """
        Extract year and month from pickup datetime for partitioning.
        
        Creates two new columns 'year' and 'month' by extracting these components
        from the 'tpep_pickup_datetime' column. These columns are used for
        time-based partitioning of the output data, which improves query performance
        for time-filtered queries.
        
        Args:
            df: Input DataFrame containing a 'tpep_pickup_datetime' column in
               ISO format (YYYY-MM-DD HH:MM:SS)
            
        Returns:
            DataFrame with additional 'year' and 'month' columns
            
        Raises:
            ValueError: If the required 'tpep_pickup_datetime' column is missing
            Exception: For other errors during date extraction
        """
        try:
            logger.info("Extracting year and month from pickup datetime")
            
            # Validate required column exists
            if "tpep_pickup_datetime" not in df.columns:
                logger.error("Required column 'tpep_pickup_datetime' not found in dataframe")
                raise ValueError("Required column 'tpep_pickup_datetime' not found in dataframe")
                
            # Extract year and month
            df_with_date = (
                df.withColumn("year", col("tpep_pickup_datetime").substr(1, 4))
                .withColumn("month", col("tpep_pickup_datetime").substr(6, 2))
            )
            
            logger.debug("Added year and month columns for partitioning")
            return df_with_date
        except Exception as e:
            logger.error(f"Failed to extract date parts: {str(e)}")
            raise
    
    def optimize_for_write(self, df: DataFrame, num_partitions: int = 20) -> DataFrame:
        """
        Optimize the DataFrame for efficient writing by repartitioning.
        
        Repartitioning controls the number of output files created during the write
        operation. This helps balance between creating too many small files (which
        impacts metadata operations) and too few large files (which reduces parallelism).
        
        The optimal number of partitions depends on the data volume and the target
        storage system's characteristics.
        
        Args:
            df: Input DataFrame to repartition
            num_partitions: Target number of partitions for the DataFrame.
                           Default is 20, which works well for medium-sized datasets.
            
        Returns:
            Repartitioned DataFrame with the specified number of partitions
        """
        logger.info(f"Optimizing DataFrame with {num_partitions} partitions")
        return df.repartition(num_partitions)
    
    def write_bronze_data(self, df: DataFrame) -> bool:
        """
        Write the normalized data to the bronze layer storage.
        
        Writes the DataFrame to the configured output location using Parquet format
        with partitioning based on the PARTITION_COLUMNS. Partitioning improves
        query performance by allowing partition pruning for filtered queries.
        
        Args:
            df: DataFrame to write, with all required columns including partition columns
            
        Returns:
            True if the write operation was successful, False otherwise
            
        Raises:
            Exception: If the write operation fails due to permission issues,
                      connectivity problems, or other errors
        """
        try:
            logger.info(f"Writing bronze layer data to {self.output_path}")
            logger.info(f"Using partition columns: {', '.join(self.PARTITION_COLUMNS)}")
            
            write_to_bucket(
                df, 
                self.output_path, 
                partition_columns=self.PARTITION_COLUMNS
            )
            
            logger.info("Successfully wrote data to bronze layer")
            return True
        except Exception as e:
            logger.error(f"Failed to write data to bronze layer: {str(e)}")
            raise
    
    def process(self) -> bool:
        """
        Execute the full bronze layer ingestion pipeline.
        
        This method orchestrates the entire ingestion process:
        1. Generate column mapping dictionary
        2. Read and normalize raw data
        3. Extract date parts for partitioning
        4. Optimize data distribution
        5. Write to bronze layer storage
        
        Returns:
            True if the entire pipeline executed successfully, False otherwise
            
        Raises:
            Exception: If any critical step in the pipeline fails
        """
        logger.info("Starting bronze layer ingestion pipeline")
        try:
            # Get mapping dictionary
            mapping_dict = self.get_mapping_dictionary()
            
            # Read and normalize data
            df = self.read_and_normalize_data(mapping_dict)
            if df is None:
                logger.warning("No data to process, ending pipeline")
                return False
                
            # Extract date parts for partitioning
            df = self.extract_date_parts(df)
            
            # Optimize for write
            df = self.optimize_for_write(df)
            
            # Preview data
            logger.info("Preview of normalized data:")
            df.show(5, truncate=False)
            
            # Write to bronze layer
            success = self.write_bronze_data(df)
            
            logger.info("Bronze layer ingestion pipeline completed successfully")
            return success
            
        except Exception as e:
            logger.error(f"Bronze layer ingestion pipeline failed: {str(e)}", exc_info=True)
            return False


def run_pipeline(
    input_path: str = BronzeLayerIngestion.DEFAULT_INPUT_PATH,
    output_path: str = BronzeLayerIngestion.DEFAULT_OUTPUT_PATH
) -> bool:
    """
    Main entry point for the bronze layer pipeline.
    
    This function provides a simple interface to run the bronze layer ingestion
    pipeline with configurable input and output paths. It handles the initialization
    of the BronzeLayerIngestion class and executes the process.
    
    Args:
        input_path: Directory path containing the raw TLC trip data files.
                   Defaults to the path defined in BronzeLayerIngestion.
        output_path: Destination path where bronze layer data will be written.
                    Defaults to the path defined in BronzeLayerIngestion.
        
    Returns:
        True if the pipeline executed successfully, False otherwise
    """
    logger.info("Starting bronze layer pipeline")
    try:
        # Initialize and run ingestion
        ingestion = BronzeLayerIngestion(
            input_path=input_path,
            output_path=output_path
        )
        return ingestion.process()
    except Exception as e:
        logger.error(f"Bronze layer pipeline failed: {str(e)}", exc_info=True)
        return False


# Execute pipeline if run as script
if __name__ == "__main__":
    success = run_pipeline()
    if not success:
        logger.error("Pipeline execution failed")
        exit(1)
