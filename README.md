# ğŸš• Pipeline de Processamento de Dados TLC

## ğŸŒŸ VisÃ£o Geral do Projeto

Este projeto implementa um pipeline de dados completo para processar os dados de viagens da ComissÃ£o de TÃ¡xi e Limusine (TLC) de Nova York. O pipeline abrange desde a ingestÃ£o de dados brutos atÃ© a criaÃ§Ã£o de camadas bronze e silver, com orquestraÃ§Ã£o via Airflow e tabelas Hive para anÃ¡lise.

## ğŸ—ï¸ Arquitetura do Pipeline

O pipeline segue uma arquitetura de data lakehouse com mÃºltiplas camadas:

1. **ğŸ“¥ Camada Raw (IngestÃ£o)**: LÃª arquivos de dados de origem (formato Parquet)
2. **ğŸ¥‰ Camada Bronze**: Dados normalizados com esquema padronizado
3. **ğŸ¥ˆ Camada Silver**: Dados enriquecidos e transformados para anÃ¡lise
4. **ğŸ“Š Camada de AnÃ¡lise**: Tabelas Hive para consultas analÃ­ticas

## ğŸš€ Como Executar o Projeto

### ğŸ“‹ PrÃ©-requisitos

- Python 3.8+
- Docker e Docker Compose
- Make

### ğŸ”§ ConfiguraÃ§Ã£o do Ambiente

#### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/tlc-pipeline.git
cd tlc-pipeline
```

#### 2. Crie e ative um ambiente virtual

```bash
python -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
```

#### 3. Instale o projeto como pacote

```bash
pip install -e .
```

#### 4. Inicie o ambiente de simulaÃ§Ã£o AWS com MinIO

```bash
make up
```

Este comando iniciarÃ¡ um contÃªiner Docker com MinIO que simula o S3 da AWS.

#### 5. ğŸª£ Crie bucket no MinIO

```bash
make create
```

Este comando configurarÃ¡ as credenciais necessÃ¡rias para acessar o MinIO.

### ğŸƒâ€â™‚ï¸ Executando o Pipeline

#### 1. Execute o pipeline completo

```bash
make run-pipeline
```

#### 2. Execute apenas a camada bronze

```bash
make run-bronze
```

#### 3. Execute apenas a camada silver

```bash
make run-silver
```

### ğŸ§ª Executando Testes

```bash
make test
```

### ğŸ§¹ Limpando o Ambiente

```bash
make clean
```

Este comando removerÃ¡ os contÃªineres Docker e limparÃ¡ os dados temporÃ¡rios.

## ğŸ§© Componentes Principais

### ğŸ“¥ IngestÃ£o de Dados Brutos

O processo de ingestÃ£o bruta lÃª arquivos Parquet de um local de origem e os prepara para processamento adicional.

#### ğŸ”„ Mapeamento de Colunas (De-Para)

O pipeline inclui um sistema sofisticado de mapeamento de colunas que:

- ğŸ“ Normaliza nomes de colunas em diferentes fontes de dados
- ğŸ” Trata colunas especiais/exclusivas que precisam de tratamento individual
- ğŸ”— Agrupa colunas semelhantes com base em mÃ©tricas de similaridade

```python
# Exemplo de uso:
de_para = sugerir_de_para(spark, "caminho/para/arquivos/parquet", limite_similaridade=0.8)
```

#### ğŸ› ï¸ FunÃ§Ãµes de Mapeamento de Colunas

1. **`sugerir_de_para()`**: Analisa arquivos Parquet para sugerir mapeamentos de colunas com base na similaridade
  - ğŸš« Trata arquivos com erro movendo-os para quarentena
  - ğŸ·ï¸ Suporta colunas exclusivas que nÃ£o devem ser agrupadas
  - ğŸ” Usa correspondÃªncia aproximada para identificar colunas semelhantes

2. **`corrigir_de_para_colunas_exclusivas()`**: Refina mapeamentos de colunas separando colunas exclusivas
  - âœ… Garante que colunas obrigatÃ³rias como "tpep_pickup_datetime" e "vendorid" sejam incluÃ­das
  - ğŸ”€ Cria mapeamentos individuais para colunas exclusivas

3. **`exportar_de_para_json()`**: Exporta mapeamentos de colunas para formato JSON para referÃªncia e auditoria

### ğŸ“Š NormalizaÃ§Ã£o de Esquema

A funÃ§Ã£o `ler_e_padronizar_parquets()` Ã© responsÃ¡vel por:

- ğŸ“‹ Ler mÃºltiplos arquivos Parquet com esquemas diferentes
- ğŸ”„ Aplicar o mapeamento de colunas para padronizar os nomes
- ğŸ·ï¸ Adicionar metadados como tipo de fonte e arquivo de origem
- ğŸ” Extrair informaÃ§Ãµes de data a partir dos nomes dos arquivos

### ğŸ¥‰ Camada Bronze

A camada bronze processa os dados brutos e:

- ğŸ§¹ Limpa e padroniza os dados
- ğŸ“Š Aplica o esquema consistente em todos os dados
- ğŸ” Adiciona metadados de processamento
- ğŸ“‚ Organiza os dados em partiÃ§Ãµes eficientes

### ğŸ¥ˆ Camada Silver

A camada silver enriquece os dados da camada bronze:

- ğŸ” Aplica regras de negÃ³cio e transformaÃ§Ãµes
- ğŸ§® Calcula mÃ©tricas e agregaÃ§Ãµes
- ğŸ”— Cria relacionamentos entre diferentes conjuntos de dados
- ğŸ“Š Prepara os dados para anÃ¡lise

### ğŸ“ SQL para Tabelas Hive

A pasta SQL contÃ©m scripts para:

- ğŸ“Š Criar tabelas externas Hive apontando para os dados processados
- ğŸ” Definir particionamento e esquemas para consultas eficientes
- ğŸ§® Criar views para facilitar anÃ¡lises comuns

### ğŸ”„ OrquestraÃ§Ã£o com Airflow

Os DAGs do Airflow orquestram todo o pipeline:

- â±ï¸ Agendamento de execuÃ§Ãµes periÃ³dicas
- ğŸ”„ Gerenciamento de dependÃªncias entre tarefas
- ğŸ“Š Monitoramento de execuÃ§Ã£o
- âš ï¸ Tratamento de falhas e retentativas

### ğŸ’¾ Escrita de Dados

A funÃ§Ã£o `write_to_raw_bucket()` lida com a escrita de dados processados no armazenamento de destino:

- ğŸ“Š Particiona dados por tipo de fonte, ano e mÃªs para consultas eficientes
- ğŸ›¡ï¸ Trata dataframes vazios adequadamente
- âš ï¸ Fornece tratamento de erros e registro de logs
- ğŸ”„ Suporta caminhos locais e S3

## ğŸ”„ Fluxo de Dados Completo

1. ğŸ“¥ **IngestÃ£o**: Arquivos Parquet de origem sÃ£o lidos
2. ğŸ”„ **NormalizaÃ§Ã£o**: Esquemas sÃ£o padronizados usando o sistema de-para
3. ğŸ¥‰ **Bronze**: Dados normalizados sÃ£o escritos na camada bronze
4. ğŸ¥ˆ **Silver**: Dados sÃ£o transformados e enriquecidos na camada silver
5. ğŸ“Š **AnÃ¡lise**: Tabelas Hive sÃ£o criadas ou atualizadas para consulta

## âš™ï¸ AutomaÃ§Ã£o com Makefile

O projeto inclui um Makefile para automatizar tarefas comuns:

- ğŸ”§ ConfiguraÃ§Ã£o do ambiente de desenvolvimento
- ğŸ§ª ExecuÃ§Ã£o de testes
- ğŸ“¦ ConstruÃ§Ã£o de pacotes
- ğŸš€ ImplantaÃ§Ã£o em diferentes ambientes

```bash
# Exemplos de comandos make
make setup      # Configura o ambiente
make test       # Executa testes
make build      # ConstrÃ³i o pacote
make deploy     # Implanta o pipeline
```

## âš ï¸ Tratamento de Erros

O pipeline inclui tratamento robusto de erros:
- ğŸ”’ Arquivos que nÃ£o podem ser processados sÃ£o movidos para um diretÃ³rio de quarentena
- ğŸ“ Mensagens de erro detalhadas sÃ£o registradas
- ğŸ›¡ï¸ Dataframes vazios sÃ£o tratados adequadamente
- ğŸ”„ Falhas em DAGs sÃ£o tratadas com polÃ­ticas de retentativa

## ğŸš€ Exemplos de Uso

```python
# 1. Inicializar sessÃ£o Spark
spark = SparkSession.builder.appName("TLC Pipeline").getOrCreate()

# 2. Gerar mapeamentos de colunas
de_para = sugerir_de_para(spark, "dados_entrada/", 
                        limite_similaridade=0.8,
                        colunas_exclusivas=["coluna_especial1", "coluna_especial2"])

# 3. Refinar mapeamentos para colunas exclusivas
de_para_refinado = corrigir_de_para_colunas_exclusivas(de_para, 
                                                    colunas_exclusivas=["coluna_especial1", "coluna_especial2"])

# 4. Exportar mapeamentos para referÃªncia
exportar_de_para_json(de_para_refinado, "mapeamentos/mapeamento_colunas.json")

# 5. Ler e padronizar os dados
df_padronizado = ler_e_padronizar_parquets(spark, "dados_entrada/", de_para_refinado)

# 6. Processar e escrever dados na camada bronze
write_to_raw_bucket(df_padronizado, "s3://bucket-bronze/dados_tlc/")

# 7. Transformar para camada silver
df_silver = transformar_para_silver(df_padronizado)
write_to_silver(df_silver, "s3://bucket-silver/dados_tlc/")
```

## ğŸ’¡ Melhores PrÃ¡ticas

- ğŸ‘€ Sempre revise os mapeamentos de colunas gerados antes de usÃ¡-los em produÃ§Ã£o
- ğŸ” Monitore o diretÃ³rio de quarentena para arquivos problemÃ¡ticos
- ğŸ›ï¸ Ajuste o limite de similaridade com base nas caracterÃ­sticas dos seus dados
- âœ… Inclua todas as colunas obrigatÃ³rias em seu mapeamento
- ğŸ“Š Verifique a qualidade dos dados apÃ³s o processamento
- ğŸ”„ Use o Airflow para monitorar a execuÃ§Ã£o do pipeline

## ğŸ“Š Estrutura do Projeto

```
.
â”œâ”€â”€ Makefile                      # AutomaÃ§Ã£o de tarefas
â”œâ”€â”€ README.md                     # DocumentaÃ§Ã£o do projeto
â”œâ”€â”€ dags/                         # DAGs do Airflow para orquestraÃ§Ã£o
â”‚   â”œâ”€â”€ dag.py         # DAG para processamento das camada bronze e silver
â”œâ”€â”€ sql/                          # Scripts SQL para tabelas Hive
â”‚   â”œâ”€â”€ create_table_hive.sql     # CriaÃ§Ã£o de tabelas na camada silver
â””â”€â”€ src/
   â””â”€â”€ tlc_pipeline/
       â”œâ”€â”€ bronze/               # Processamento da camada bronze
       â”‚   â””â”€â”€ bronze_layer.py
       â”œâ”€â”€ raw_ingestion/        # IngestÃ£o de dados brutos
       â”‚   â”œâ”€â”€ de_para.py        # Funcionalidade de mapeamento de colunas
       â”‚   â”œâ”€â”€ normalize_schema.py # NormalizaÃ§Ã£o de esquemas
       â”‚   â””â”€â”€ write_to_raw_bucket.py # Funcionalidade de escrita de dados
       â””â”€â”€ silver/               # Processamento da camada silver
           â””â”€â”€ silver_layer.py
```

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT - consulte o arquivo LICENSE para obter detalhes.
```

Para criar este arquivo, execute:

```bash
mkdir -p $(dirname README.md)
touch README.md